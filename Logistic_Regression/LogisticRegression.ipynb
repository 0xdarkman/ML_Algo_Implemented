{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from log_lr import LogisticRegression as lgr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load digit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.3, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try sklearn logistic regression implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "slr = StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = slr.transform(X_train)\n",
    "X_test_scaled = slr.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=17, max_iter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=200,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=17, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = lr.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets try our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1 = lgr(iters=200, lr=0.01, random_seed=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 152.05285592019163\n",
      "Epoch: 2, Loss: 144.2369993604498\n",
      "Epoch: 3, Loss: 138.5856261467034\n",
      "Epoch: 4, Loss: 133.9721275536351\n",
      "Epoch: 5, Loss: 130.17383370623173\n",
      "Epoch: 6, Loss: 126.86195557173528\n",
      "Epoch: 7, Loss: 123.90353109572844\n",
      "Epoch: 8, Loss: 121.19280694070706\n",
      "Epoch: 9, Loss: 118.67671302547835\n",
      "Epoch: 10, Loss: 116.32274425462292\n",
      "Epoch: 11, Loss: 114.10918864540227\n",
      "Epoch: 12, Loss: 112.02127969175251\n",
      "Epoch: 13, Loss: 110.04397478591818\n",
      "Epoch: 14, Loss: 108.16442244485886\n",
      "Epoch: 15, Loss: 106.36958488240121\n",
      "Epoch: 16, Loss: 104.64880843762386\n",
      "Epoch: 17, Loss: 102.99298279701803\n",
      "Epoch: 18, Loss: 101.39522472528485\n",
      "Epoch: 19, Loss: 99.84995851766124\n",
      "Epoch: 20, Loss: 98.35289427412951\n",
      "Epoch: 21, Loss: 96.90053124935744\n",
      "Epoch: 22, Loss: 95.4900767039908\n",
      "Epoch: 23, Loss: 94.11920561133273\n",
      "Epoch: 24, Loss: 92.78599644156435\n",
      "Epoch: 25, Loss: 91.48881338629863\n",
      "Epoch: 26, Loss: 90.22625985804855\n",
      "Epoch: 27, Loss: 88.9971157091173\n",
      "Epoch: 28, Loss: 87.80030509573439\n",
      "Epoch: 29, Loss: 86.63486169952205\n",
      "Epoch: 30, Loss: 85.49990809119869\n",
      "Epoch: 31, Loss: 84.39463621155818\n",
      "Epoch: 32, Loss: 83.31829461108909\n",
      "Epoch: 33, Loss: 82.2701772602702\n",
      "Epoch: 34, Loss: 81.2496157153055\n",
      "Epoch: 35, Loss: 80.25597254963674\n",
      "Epoch: 36, Loss: 79.28863655887076\n",
      "Epoch: 37, Loss: 78.34701886679534\n",
      "Epoch: 38, Loss: 77.4305500235424\n",
      "Epoch: 39, Loss: 76.53867769709257\n",
      "Epoch: 40, Loss: 75.67086492181922\n",
      "Epoch: 41, Loss: 74.82658870114975\n",
      "Epoch: 42, Loss: 74.00533890716578\n",
      "Epoch: 43, Loss: 73.20661737128658\n",
      "Epoch: 44, Loss: 72.42993712483366\n",
      "Epoch: 45, Loss: 71.67482173851698\n",
      "Epoch: 46, Loss: 70.94080473959222\n",
      "Epoch: 47, Loss: 70.2274290849785\n",
      "Epoch: 48, Loss: 69.53424668131636\n",
      "Epoch: 49, Loss: 68.86081794314562\n",
      "Epoch: 50, Loss: 68.20671138509947\n",
      "Epoch: 51, Loss: 67.57150324379049\n",
      "Epoch: 52, Loss: 66.95477712667616\n",
      "Epoch: 53, Loss: 66.35612368503504\n",
      "Epoch: 54, Loss: 65.77514030891835\n",
      "Epoch: 55, Loss: 65.2114308420905\n",
      "Epoch: 56, Loss: 64.66460531554608\n",
      "Epoch: 57, Loss: 64.13427969853763\n",
      "Epoch: 58, Loss: 63.62007566653853\n",
      "Epoch: 59, Loss: 63.12162038591077\n",
      "Epoch: 60, Loss: 62.63854631538435\n",
      "Epoch: 61, Loss: 62.17049102466812\n",
      "Epoch: 62, Loss: 61.717097030661535\n",
      "Epoch: 63, Loss: 61.27801165178261\n",
      "Epoch: 64, Loss: 60.85288688090571\n",
      "Epoch: 65, Loss: 60.4413792773046\n",
      "Epoch: 66, Loss: 60.043149877846744\n",
      "Epoch: 67, Loss: 59.65786412748617\n",
      "Epoch: 68, Loss: 59.28519182887493\n",
      "Epoch: 69, Loss: 58.92480711066318\n",
      "Epoch: 70, Loss: 58.576388413801226\n",
      "Epoch: 71, Loss: 58.23961849490169\n",
      "Epoch: 72, Loss: 57.91418444547938\n",
      "Epoch: 73, Loss: 57.59977772566701\n",
      "Epoch: 74, Loss: 57.29609421081685\n",
      "Epoch: 75, Loss: 57.00283424924598\n",
      "Epoch: 76, Loss: 56.71970272927151\n",
      "Epoch: 77, Loss: 56.446409153613125\n",
      "Epoch: 78, Loss: 56.18266771921457\n",
      "Epoch: 79, Loss: 55.92819740055058\n",
      "Epoch: 80, Loss: 55.68272203453937\n",
      "Epoch: 81, Loss: 55.445970405267005\n",
      "Epoch: 82, Loss: 55.21767632684531\n",
      "Epoch: 83, Loss: 54.997578722862045\n",
      "Epoch: 84, Loss: 54.78542170103674\n",
      "Epoch: 85, Loss: 54.580954621860606\n",
      "Epoch: 86, Loss: 54.383932160170914\n",
      "Epoch: 87, Loss: 54.19411435878293\n",
      "Epoch: 88, Loss: 54.01126667347357\n",
      "Epoch: 89, Loss: 53.835160008776015\n",
      "Epoch: 90, Loss: 53.66557074420161\n",
      "Epoch: 91, Loss: 53.502280750652886\n",
      "Epoch: 92, Loss: 53.34507739692685\n",
      "Epoch: 93, Loss: 53.19375354633198\n",
      "Epoch: 94, Loss: 53.0481075435523\n",
      "Epoch: 95, Loss: 52.90794319199021\n",
      "Epoch: 96, Loss: 52.773069721904356\n",
      "Epoch: 97, Loss: 52.64330174973079\n",
      "Epoch: 98, Loss: 52.518459229035635\n",
      "Epoch: 99, Loss: 52.39836739359552\n",
      "Epoch: 100, Loss: 52.28285669313921\n",
      "Epoch: 101, Loss: 52.17176272231069\n",
      "Epoch: 102, Loss: 52.06492614343166\n",
      "Epoch: 103, Loss: 51.96219260365009\n",
      "Epoch: 104, Loss: 51.86341264706303\n",
      "Epoch: 105, Loss: 51.76844162239626\n",
      "Epoch: 106, Loss: 51.6771395868125\n",
      "Epoch: 107, Loss: 51.58937120640315\n",
      "Epoch: 108, Loss: 51.505005653898685\n",
      "Epoch: 109, Loss: 51.423916504108824\n",
      "Epoch: 110, Loss: 51.34598162757749\n",
      "Epoch: 111, Loss: 51.27108308290917\n",
      "Epoch: 112, Loss: 51.199107008194055\n",
      "Epoch: 113, Loss: 51.12994351192861\n",
      "Epoch: 114, Loss: 51.063486563797994\n",
      "Epoch: 115, Loss: 50.99963388565619\n",
      "Epoch: 116, Loss: 50.938286843009394\n",
      "Epoch: 117, Loss: 50.879350337279575\n",
      "Epoch: 118, Loss: 50.82273269909627\n",
      "Epoch: 119, Loss: 50.76834558283803\n",
      "Epoch: 120, Loss: 50.71610386261905\n",
      "Epoch: 121, Loss: 50.665925529892206\n",
      "Epoch: 122, Loss: 50.61773159281696\n",
      "Epoch: 123, Loss: 50.57144597751935\n",
      "Epoch: 124, Loss: 50.5269954313514\n",
      "Epoch: 125, Loss: 50.4843094282389\n",
      "Epoch: 126, Loss: 50.443320076190474\n",
      "Epoch: 127, Loss: 50.40396202702416\n",
      "Epoch: 128, Loss: 50.36617238835544\n",
      "Epoch: 129, Loss: 50.329890637876574\n",
      "Epoch: 130, Loss: 50.29505853994648\n",
      "Epoch: 131, Loss: 50.26162006450023\n",
      "Epoch: 132, Loss: 50.229521308277576\n",
      "Epoch: 133, Loss: 50.198710418362396\n",
      "Epoch: 134, Loss: 50.16913751801755\n",
      "Epoch: 135, Loss: 50.140754634793375\n",
      "Epoch: 136, Loss: 50.113515630882645\n",
      "Epoch: 137, Loss: 50.08737613569006\n",
      "Epoch: 138, Loss: 50.06229348058049\n",
      "Epoch: 139, Loss: 50.038226635766385\n",
      "Epoch: 140, Loss: 50.015136149292175\n",
      "Epoch: 141, Loss: 49.992984088071346\n",
      "Epoch: 142, Loss: 49.971733980929095\n",
      "Epoch: 143, Loss: 49.951350763603145\n",
      "Epoch: 144, Loss: 49.93180072565323\n",
      "Epoch: 145, Loss: 49.913051459229294\n",
      "Epoch: 146, Loss: 49.89507180964798\n",
      "Epoch: 147, Loss: 49.87783182772665\n",
      "Epoch: 148, Loss: 49.861302723824124\n",
      "Epoch: 149, Loss: 49.84545682353735\n",
      "Epoch: 150, Loss: 49.830267525003975\n",
      "Epoch: 151, Loss: 49.815709257760766\n",
      "Epoch: 152, Loss: 49.80175744310876\n",
      "Epoch: 153, Loss: 49.78838845593686\n",
      "Epoch: 154, Loss: 49.77557958795591\n",
      "Epoch: 155, Loss: 49.763309012297015\n",
      "Epoch: 156, Loss: 49.75155574942801\n",
      "Epoch: 157, Loss: 49.74029963434376\n",
      "Epoch: 158, Loss: 49.729521284986525\n",
      "Epoch: 159, Loss: 49.71920207185413\n",
      "Epoch: 160, Loss: 49.70932408875453\n",
      "Epoch: 161, Loss: 49.699870124666994\n",
      "Epoch: 162, Loss: 49.690823636670636\n",
      "Epoch: 163, Loss: 49.68216872390289\n",
      "Epoch: 164, Loss: 49.6738901025113\n",
      "Epoch: 165, Loss: 49.66597308156325\n",
      "Epoch: 166, Loss: 49.6584035398795\n",
      "Epoch: 167, Loss: 49.651167903758356\n",
      "Epoch: 168, Loss: 49.64425312555863\n",
      "Epoch: 169, Loss: 49.63764666311032\n",
      "Epoch: 170, Loss: 49.63133645992329\n",
      "Epoch: 171, Loss: 49.62531092616488\n",
      "Epoch: 172, Loss: 49.61955892037868\n",
      "Epoch: 173, Loss: 49.614069731917375\n",
      "Epoch: 174, Loss: 49.608833064063354\n",
      "Epoch: 175, Loss: 49.60383901781201\n",
      "Epoch: 176, Loss: 49.59907807629303\n",
      "Epoch: 177, Loss: 49.594541089805894\n",
      "Epoch: 178, Loss: 49.59021926144655\n",
      "Epoch: 179, Loss: 49.586104133302946\n",
      "Epoch: 180, Loss: 49.58218757319756\n",
      "Epoch: 181, Loss: 49.57846176195608\n",
      "Epoch: 182, Loss: 49.57491918118159\n",
      "Epoch: 183, Loss: 49.57155260151468\n",
      "Epoch: 184, Loss: 49.56835507136004\n",
      "Epoch: 185, Loss: 49.56531990606118\n",
      "Epoch: 186, Loss: 49.56244067750513\n",
      "Epoch: 187, Loss: 49.55971120413977\n",
      "Epoch: 188, Loss: 49.55712554138706\n",
      "Epoch: 189, Loss: 49.55467797243604\n",
      "Epoch: 190, Loss: 49.55236299939987\n",
      "Epoch: 191, Loss: 49.550175334822164\n",
      "Epoch: 192, Loss: 49.54810989351828\n",
      "Epoch: 193, Loss: 49.546161784737684\n",
      "Epoch: 194, Loss: 49.5443263046345\n",
      "Epoch: 195, Loss: 49.542598929033595\n",
      "Epoch: 196, Loss: 49.54097530648035\n",
      "Epoch: 197, Loss: 49.539451251562646\n",
      "Epoch: 198, Loss: 49.53802273849446\n",
      "Epoch: 199, Loss: 49.5366858949506\n",
      "Epoch: 200, Loss: 49.535436996142806\n"
     ]
    }
   ],
   "source": [
    "lr1.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = lr1.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9648148148148148"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr1.losses)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "canc = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(canc.data, canc.target, test_size=0.3, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "slr = StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = slr.transform(X_train)\n",
    "X_test_scaled = slr.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try sklearn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=17, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = lr.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9766081871345029"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1 = lgr(lr=0.01, random_seed=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 16.3157145918247\n",
      "Epoch: 2, Loss: 14.870298476255394\n",
      "Epoch: 3, Loss: 14.083408310826915\n",
      "Epoch: 4, Loss: 13.542751604077086\n",
      "Epoch: 5, Loss: 13.134106120371598\n",
      "Epoch: 6, Loss: 12.790345492977584\n",
      "Epoch: 7, Loss: 12.486356792296338\n",
      "Epoch: 8, Loss: 12.207087804204244\n",
      "Epoch: 9, Loss: 11.946006676234285\n",
      "Epoch: 10, Loss: 11.700177619562082\n",
      "Epoch: 11, Loss: 11.467905813765832\n",
      "Epoch: 12, Loss: 11.247993663630828\n",
      "Epoch: 13, Loss: 11.03949385889564\n",
      "Epoch: 14, Loss: 10.841610834845259\n",
      "Epoch: 15, Loss: 10.653652968922136\n",
      "Epoch: 16, Loss: 10.475005322336145\n",
      "Epoch: 17, Loss: 10.305112355515044\n",
      "Epoch: 18, Loss: 10.143466298846468\n",
      "Epoch: 19, Loss: 9.989599074652531\n",
      "Epoch: 20, Loss: 9.843076569349739\n",
      "Epoch: 21, Loss: 9.70349449202609\n",
      "Epoch: 22, Loss: 9.570475304984566\n",
      "Epoch: 23, Loss: 9.443665871772781\n",
      "Epoch: 24, Loss: 9.322735578161705\n",
      "Epoch: 25, Loss: 9.207374759643699\n",
      "Epoch: 26, Loss: 9.097293325079898\n",
      "Epoch: 27, Loss: 8.992219506124817\n",
      "Epoch: 28, Loss: 8.891898690046858\n",
      "Epoch: 29, Loss: 8.796092312526907\n",
      "Epoch: 30, Loss: 8.70457679926614\n",
      "Epoch: 31, Loss: 8.617142552614009\n",
      "Epoch: 32, Loss: 8.533592983406672\n",
      "Epoch: 33, Loss: 8.45374358992886\n",
      "Epoch: 34, Loss: 8.377421086238018\n",
      "Epoch: 35, Loss: 8.304462581635104\n",
      "Epoch: 36, Loss: 8.234714812250568\n",
      "Epoch: 37, Loss: 8.168033424803006\n",
      "Epoch: 38, Loss: 8.104282311735789\n",
      "Epoch: 39, Loss: 8.043332996221158\n",
      "Epoch: 40, Loss: 7.985064064969255\n",
      "Epoch: 41, Loss: 7.929360646389193\n",
      "Epoch: 42, Loss: 7.87611393140342\n",
      "Epoch: 43, Loss: 7.82522073409052\n",
      "Epoch: 44, Loss: 7.776583089300258\n",
      "Epoch: 45, Loss: 7.7301078844247755\n",
      "Epoch: 46, Loss: 7.6857065226008485\n",
      "Epoch: 47, Loss: 7.643294614744047\n",
      "Epoch: 48, Loss: 7.602791697962948\n",
      "Epoch: 49, Loss: 7.564120978060817\n",
      "Epoch: 50, Loss: 7.527209093995653\n",
      "Epoch: 51, Loss: 7.491985902332263\n",
      "Epoch: 52, Loss: 7.458384279878116\n",
      "Epoch: 53, Loss: 7.426339942846066\n",
      "Epoch: 54, Loss: 7.395791281029688\n",
      "Epoch: 55, Loss: 7.366679205610664\n",
      "Epoch: 56, Loss: 7.338947009341484\n",
      "Epoch: 57, Loss: 7.312540237961219\n",
      "Epoch: 58, Loss: 7.287406571806953\n",
      "Epoch: 59, Loss: 7.263495716679806\n",
      "Epoch: 60, Loss: 7.240759303111798\n",
      "Epoch: 61, Loss: 7.219150793259763\n",
      "Epoch: 62, Loss: 7.19862539472469\n",
      "Epoch: 63, Loss: 7.1791399806607075\n",
      "Epoch: 64, Loss: 7.160653015597152\n",
      "Epoch: 65, Loss: 7.143124486451159\n",
      "Epoch: 66, Loss: 7.126515838256803\n",
      "Epoch: 67, Loss: 7.110789914180969\n",
      "Epoch: 68, Loss: 7.095910899435992\n",
      "Epoch: 69, Loss: 7.081844268735321\n",
      "Epoch: 70, Loss: 7.068556736971225\n",
      "Epoch: 71, Loss: 7.0560162128233515\n",
      "Epoch: 72, Loss: 7.04419175503398\n",
      "Epoch: 73, Loss: 7.03305353111046\n",
      "Epoch: 74, Loss: 7.022572778237758\n",
      "Epoch: 75, Loss: 7.012721766204472\n",
      "Epoch: 76, Loss: 7.0034737621643774\n",
      "Epoch: 77, Loss: 6.994802997072632\n",
      "Epoch: 78, Loss: 6.986684633651455\n",
      "Epoch: 79, Loss: 6.979094735754226\n",
      "Epoch: 80, Loss: 6.972010239010229\n",
      "Epoch: 81, Loss: 6.965408922644072\n",
      "Epoch: 82, Loss: 6.959269382374807\n",
      "Epoch: 83, Loss: 6.953571004309799\n",
      "Epoch: 84, Loss: 6.948293939757419\n",
      "Epoch: 85, Loss: 6.943419080891042\n",
      "Epoch: 86, Loss: 6.938928037204253\n",
      "Epoch: 87, Loss: 6.934803112704138\n",
      "Epoch: 88, Loss: 6.931027283795568\n",
      "Epoch: 89, Loss: 6.927584177815081\n",
      "Epoch: 90, Loss: 6.924458052177889\n",
      "Epoch: 91, Loss: 6.92163377410604\n",
      "Epoch: 92, Loss: 6.919096800909747\n",
      "Epoch: 93, Loss: 6.9168331607973945\n",
      "Epoch: 94, Loss: 6.914829434192888\n",
      "Epoch: 95, Loss: 6.913072735541722\n",
      "Epoch: 96, Loss: 6.911550695589463\n",
      "Epoch: 97, Loss: 6.910251444118482\n",
      "Epoch: 98, Loss: 6.9091635931305015\n",
      "Epoch: 99, Loss: 6.90827622046393\n",
      "Epoch: 100, Loss: 6.907578853836382\n"
     ]
    }
   ],
   "source": [
    "lr1.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = lr1.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9766081871345029"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.97        61\n",
      "           1       0.96      1.00      0.98       110\n",
      "\n",
      "    accuracy                           0.98       171\n",
      "   macro avg       0.98      0.97      0.97       171\n",
      "weighted avg       0.98      0.98      0.98       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pandas with another dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem\n",
    "\n",
    "Predict the presence or absence of cardiovascular disease (CVD) using the patient examination results.\n",
    "\n",
    "#### Data description\n",
    "\n",
    "There are 3 types of input features:\n",
    "\n",
    "- *Objective*: factual information;\n",
    "- *Examination*: results of medical examination;\n",
    "- *Subjective*: information given by the patient.\n",
    "\n",
    "| Feature | Variable Type | Variable      | Value Type |\n",
    "|---------|--------------|---------------|------------|\n",
    "| Age | Objective Feature | age | int (days) |\n",
    "| Height | Objective Feature | height | int (cm) |\n",
    "| Weight | Objective Feature | weight | float (kg) |\n",
    "| Gender | Objective Feature | gender | categorical code |\n",
    "| Systolic blood pressure | Examination Feature | ap_hi | int |\n",
    "| Diastolic blood pressure | Examination Feature | ap_lo | int |\n",
    "| Cholesterol | Examination Feature | cholesterol | 1: normal, 2: above normal, 3: well above normal |\n",
    "| Glucose | Examination Feature | gluc | 1: normal, 2: above normal, 3: well above normal |\n",
    "| Smoking | Subjective Feature | smoke | binary |\n",
    "| Alcohol intake | Subjective Feature | alco | binary |\n",
    "| Physical activity | Subjective Feature | active | binary |\n",
    "| Presence or absence of cardiovascular disease | Target Variable | cardio | binary |\n",
    "\n",
    "All of the dataset values were collected at the moment of medical examination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../DecisionTree/train.csv', index_col='id', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age_in_years'] = np.floor(df['age']/365.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['cardio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gender'] =df['gender'].apply(lambda x: x-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age_40-50'] = df['age_in_years'].apply(lambda x: 1 if x >= 40 and x < 50 else 0)\n",
    "df['Age_50-55'] = df['age_in_years'].apply(lambda x: 1 if x >= 50 and x < 55 else 0)\n",
    "df['Age_55-60'] = df['age_in_years'].apply(lambda x: 1 if x >= 55 and x < 60 else 0)\n",
    "df['Age_60-65'] = df['age_in_years'].apply(lambda x: 1 if x >= 60 and x < 65 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['aphi_120-140'] = df['ap_hi'].apply(lambda x: 1 if x >= 120 and x < 140 else 0)\n",
    "df['aphi_140-160'] = df['ap_hi'].apply(lambda x: 1 if x >= 140 and x < 160 else 0)\n",
    "df['aphi_160-180'] = df['ap_hi'].apply(lambda x: 1 if x >= 160 and x < 180 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.get_dummies(df, prefix=['cholesterol'], columns=['cholesterol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a, b):\n",
    "    return b/ (a/100)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bmi'] = df.apply(lambda x: f(x.height, x.weight), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(labels=['gluc', 'ap_lo', 'alco', 'age', 'cardio', 'age_in_years', 'ap_hi', 'height', 'weight'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bmi'] = np.floor(df['bmi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>smoke</th>\n",
       "      <th>active</th>\n",
       "      <th>Age_40-50</th>\n",
       "      <th>Age_50-55</th>\n",
       "      <th>Age_55-60</th>\n",
       "      <th>Age_60-65</th>\n",
       "      <th>aphi_120-140</th>\n",
       "      <th>aphi_140-160</th>\n",
       "      <th>aphi_160-180</th>\n",
       "      <th>cholesterol_1</th>\n",
       "      <th>cholesterol_2</th>\n",
       "      <th>cholesterol_3</th>\n",
       "      <th>bmi</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    gender  smoke  active  Age_40-50  Age_50-55  Age_55-60  Age_60-65  \\\n",
       "id                                                                      \n",
       "0        1      0       1          0          1          0          0   \n",
       "1        0      0       1          0          0          1          0   \n",
       "2        0      0       0          0          1          0          0   \n",
       "3        1      0       1          1          0          0          0   \n",
       "4        0      0       0          1          0          0          0   \n",
       "\n",
       "    aphi_120-140  aphi_140-160  aphi_160-180  cholesterol_1  cholesterol_2  \\\n",
       "id                                                                           \n",
       "0              0             0             0              1              0   \n",
       "1              0             1             0              0              0   \n",
       "2              1             0             0              0              0   \n",
       "3              0             1             0              1              0   \n",
       "4              0             0             0              1              0   \n",
       "\n",
       "    cholesterol_3   bmi  \n",
       "id                       \n",
       "0               0  21.0  \n",
       "1               1  34.0  \n",
       "2               1  23.0  \n",
       "3               0  28.0  \n",
       "4               0  23.0  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(df, labels, test_size=0.3, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "slr = StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = slr.transform(X_train)\n",
    "X_valid_scaled = slr.transform(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=17, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = lr.predict(X_valid_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7133333333333334"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_valid, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1 = lgr(random_seed=17, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.766259247549339\n",
      "Epoch: 2, Loss: 1.0910108445818634\n",
      "Epoch: 3, Loss: 1.1794903258423806\n",
      "Epoch: 4, Loss: 1.1115469644714597\n",
      "Epoch: 5, Loss: 1.2377427931534823\n",
      "Epoch: 6, Loss: 1.200621609054076\n",
      "Epoch: 7, Loss: 1.2782010968787998\n",
      "Epoch: 8, Loss: 1.257026263541388\n",
      "Epoch: 9, Loss: 1.3033013352902822\n",
      "Epoch: 10, Loss: 1.2920527435031575\n",
      "Epoch: 11, Loss: 1.3199681594786572\n",
      "Epoch: 12, Loss: 1.3149227418149367\n",
      "Epoch: 13, Loss: 1.332412682568752\n",
      "Epoch: 14, Loss: 1.3311834498933295\n",
      "Epoch: 15, Loss: 1.342845960819142\n",
      "Epoch: 16, Loss: 1.343927136629403\n",
      "Epoch: 17, Loss: 1.3523537139467123\n",
      "Epoch: 18, Loss: 1.3548105648913231\n",
      "Epoch: 19, Loss: 1.3614376103905568\n",
      "Epoch: 20, Loss: 1.3646951911991498\n",
      "Epoch: 21, Loss: 1.3703092802760763\n",
      "Epoch: 22, Loss: 1.3740150153151798\n",
      "Epoch: 23, Loss: 1.3790419998677268\n",
      "Epoch: 24, Loss: 1.3829792569046735\n",
      "Epoch: 25, Loss: 1.3876465745062874\n",
      "Epoch: 26, Loss: 1.3916814432951916\n",
      "Epoch: 27, Loss: 1.3961082975065602\n",
      "Epoch: 28, Loss: 1.400157231655415\n",
      "Epoch: 29, Loss: 1.4044043779846613\n",
      "Epoch: 30, Loss: 1.4084146625247604\n",
      "Epoch: 31, Loss: 1.4125117563604075\n",
      "Epoch: 32, Loss: 1.4164497516375296\n",
      "Epoch: 33, Loss: 1.4204102883657268\n",
      "Epoch: 34, Loss: 1.4242543471218303\n",
      "Epoch: 35, Loss: 1.428083761255682\n",
      "Epoch: 36, Loss: 1.4318199390630975\n",
      "Epoch: 37, Loss: 1.4355199421045692\n",
      "Epoch: 38, Loss: 1.4391393735301299\n",
      "Epoch: 39, Loss: 1.4427102313948503\n",
      "Epoch: 40, Loss: 1.446207500079285\n",
      "Epoch: 41, Loss: 1.449649188179924\n",
      "Epoch: 42, Loss: 1.4530212923486792\n",
      "Epoch: 43, Loss: 1.4563340449299587\n",
      "Epoch: 44, Loss: 1.4595797225049134\n",
      "Epoch: 45, Loss: 1.4627642599997164\n",
      "Epoch: 46, Loss: 1.4658835337297693\n",
      "Epoch: 47, Loss: 1.468941123364325\n",
      "Epoch: 48, Loss: 1.4719349831262973\n",
      "Epoch: 49, Loss: 1.4748674170995582\n",
      "Epoch: 50, Loss: 1.477737589889355\n",
      "Epoch: 51, Loss: 1.4805471264106083\n",
      "Epoch: 52, Loss: 1.4832959041642606\n",
      "Epoch: 53, Loss: 1.485985195203138\n",
      "Epoch: 54, Loss: 1.488615302144963\n",
      "Epoch: 55, Loss: 1.4911873200472985\n",
      "Epoch: 56, Loss: 1.4937018081025273\n",
      "Epoch: 57, Loss: 1.4961597768927588\n",
      "Epoch: 58, Loss: 1.498561941783378\n",
      "Epoch: 59, Loss: 1.500909275596031\n",
      "Epoch: 60, Loss: 1.503202588704768\n",
      "Epoch: 61, Loss: 1.5054428380289162\n",
      "Epoch: 62, Loss: 1.5076308906335034\n",
      "Epoch: 63, Loss: 1.5097676961771604\n",
      "Epoch: 64, Loss: 1.511854153613048\n",
      "Epoch: 65, Loss: 1.5138912071921578\n",
      "Epoch: 66, Loss: 1.5158797711227248\n",
      "Epoch: 67, Loss: 1.5178207828280106\n",
      "Epoch: 68, Loss: 1.5197151602189392\n",
      "Epoch: 69, Loss: 1.5215638310910198\n",
      "Epoch: 70, Loss: 1.5233677087774007\n",
      "Epoch: 71, Loss: 1.525127708259882\n",
      "Epoch: 72, Loss: 1.5268447322066176\n",
      "Epoch: 73, Loss: 1.5285196797129772\n",
      "Epoch: 74, Loss: 1.5301534382286575\n",
      "Epoch: 75, Loss: 1.5317468882330658\n",
      "Epoch: 76, Loss: 1.5333008985214418\n",
      "Epoch: 77, Loss: 1.5348163286570253\n",
      "Epoch: 78, Loss: 1.5362940261889506\n",
      "Epoch: 79, Loss: 1.537734827905062\n",
      "Epoch: 80, Loss: 1.5391395581740304\n",
      "Epoch: 81, Loss: 1.5405090295652002\n",
      "Epoch: 82, Loss: 1.5418440418558088\n",
      "Epoch: 83, Loss: 1.543145382328915\n",
      "Epoch: 84, Loss: 1.5444138251828112\n",
      "Epoch: 85, Loss: 1.545650131675925\n",
      "Epoch: 86, Loss: 1.546855049786255\n",
      "Epoch: 87, Loss: 1.5480293142932644\n",
      "Epoch: 88, Loss: 1.549173646597918\n",
      "Epoch: 89, Loss: 1.550288754788071\n",
      "Epoch: 90, Loss: 1.5513753335654243\n",
      "Epoch: 91, Loss: 1.5524340643171026\n",
      "Epoch: 92, Loss: 1.5534656151163957\n",
      "Epoch: 93, Loss: 1.5544706408103668\n",
      "Epoch: 94, Loss: 1.55544978307319\n",
      "Epoch: 95, Loss: 1.556403670512866\n",
      "Epoch: 96, Loss: 1.557332918763035\n",
      "Epoch: 97, Loss: 1.5582381306084367\n",
      "Epoch: 98, Loss: 1.5591198961053776\n",
      "Epoch: 99, Loss: 1.559978792723956\n",
      "Epoch: 100, Loss: 1.560815385490049\n"
     ]
    }
   ],
   "source": [
    "lr1.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = lr1.predict(X_valid_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7133809523809523"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_valid, preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('pytorch1': conda)",
   "language": "python",
   "name": "python37664bitpytorch1conda2975089d5f5544148d24115825f13c3f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
